# 评分卡建模流程
# 简介  
本文用于内部评分建模，包括以下几点：  

> 一、样本选择  
> 二、特征工程  
> 三、模型选择  
> 四、结果保存  
> 五、其他想法  
> 六、总结  

评分卡建模流程可以参考：  
[自己总结的评分卡建模流程](https://blog.csdn.net/q337100/article/details/80693548)  
[评分卡建模python代码(转)](https://mp.weixin.qq.com/s?__biz=MzI5NDY1MjQzNA%3D%3D&idx=1&mid=2247485060&sn=0c731277f0b4593de9dc5dac2dc67fef)  


## 一、样本选择  
> **1、坏样本不够:** 由于业务发展等原因，刚开始数据量可能不支持建模，可以选择使用先专家模型。等到沉淀了一定数据量，在进行独立的模型开发。当总的数据量足够时，也需要保证坏样本的数据量超过一定数量，以此确保模型的稳定性。如果坏样本数量不够，也可以使用一些方法获取更多的坏样本。例如：**更改坏样本的定义，覆盖更多客户；人工通过相关黑信息关联找出来的标记样本。**  

> **2、样本有偏:** 举例：假如你发现坏客户符合某些聚集特征，你指定策略1进行打击后，这一类坏客户被控制，以后的坏客户都不具备这样的坏特征。如果你的坏样本的收集时间在在策略1上线之后，这个时候模型训练的结果可能满足聚集特征的风险低，不满足聚集特征的交易反而风险高，也就是说**聚集特征的权重是负数。**这时候模型的解释性出了问题，这个也是模型训练中一个过拟合问题的范畴。为了有效解决这个问题，可以根据业务经验来查看模型中变量的权重是否与经验相悖，如果相悖，需要仔细评估。对于是样本有偏带来的问题，可以通过重新加入符合某些条件的样本来弥补。对于这些弥补的样本获取方法**一种可以从拦截样本中选择，一个可以根据经验来人工生成样本**。

> **3、样本不平衡：** 谈谈模型的不平衡学习。风控模型学习是个典型的不平衡学习问题，他同时具备不平衡学习领域两个问题：  
>>（1）正负样本比率悬殊，但是正负类样本都足够多；  
>>（2）正样本样本个数也很稀少。第一个问题是基本满足样本在特征空间的覆盖情况，只是比率较大导致某些学习模型应用会出现问题。第二个问题是样本太少，导致样本在特征空间的覆盖很小，极容易过拟合，不能覆盖特征空间和对欺诈场景的覆盖。对于第二个问题，最好的方法还是先收集样本+一些不平衡学习方法。对于正负样本的比率问题，有的用1:1，有的人用1:10，有的说是1:13.这些大多都是经验。其实，对于比率这个问题，说到底就是负样本该采样多少的问题。我觉得只要保证负样本也尽可能多满足覆盖特征空间就好，因为很多负样本（好的交易样本）模式都是很相似的，对于相似的模式不用保留太多的样本。但是本来正样本就少，如果负样本和正样本一样多，我个人认为随机采样的负样本覆盖的特征空间会很小，所以，我个人不是很赞同1:1的比率。

> **4、拒绝推论：** 为了防止申请样本的可能偏误，进而还原申请客户的真实分布情形。方法：1.所有拒绝件都当做坏样本；2.根据现有放款客户的逾期做推论；3.根据模型分区对应的好坏比做推论。

> **5、分群分析:** 分群分析的主要目的在于寻找一适当的样本分群方式，将合格样本区分至各个分群后，分别开发个别的评分模型。分群分析主要可采取以下两种方式：  
>> **1）业务需求 (Business Sense)**    
    乃依实际业务作业流程或历史经验法则先行找出可能之分群方式，之后再藉由统计分析结果验证是否为有意义的分群。  
>> **2）统计预测力 (Predictive Power)**    
乃纯粹利用统计分析模块进行如判定树 (Decision Tree)及群集分析(Cluster) 等，藉此找出对绩效指标有预测意义的分群变量。  


## 二、特征工程  
> 特征工程包括特征衍生、特征预处理、特征筛选等过程。  

> **1、特征衍生：**  
>>[特征衍生](https://blog.csdn.net/q337100/article/details/80771963)  

> **2、特征预处理：**  
>> **1）缺失值填充：**特征的缺失值填充前，我们需要先统计特征的缺失值比率。采用某个特征来区别正常交易和异常交易前，这个特征的缺失值比率不能超过一定的阈值。对于缺失值填充的常用方法有：均值，中值，0值等。

>> **2）异常值处理：**可能由于某些原因，导致系统在收集样本时候，出现错误，特征值过大或者过小。当然，这个可能本来数据就是这样，但是，我们也需要做个处理。常用的方法：设置分位点做截断，比如0.1%,99.9%分位点等。如果对特征做了离散化处理（分箱处理），变相也是做了异常值处理。

>> **3）连续特征归一化处理：**对于连续特征，比如用户的注册时间间隔，原来的值范围各自不同，不在统一的尺度。有的连续特征值范围大，有的连续特征值范围小。如果不做归一化处理，连续特征中值范围的大的特征会淹没值范围小的连续特征对模型的影响。所以，有必要对连续特征做归一化处理。常用的连续特征归一化处理方法：（1）min-max方法；（2）z-score方法。对于互联网数据，很多特征呈现长尾power-law分布，所以，大多场景针对这种情况在做min-max 或者z-score之前，会对连续特征先做log(x)变换。  

>> **4）连续特征离散化处理：**相对连续特征归一化处理，还可以对连续特征进行离散化处理。在logistic regression中，大家经常会把连续特征做离散化处理，好处：  
>>>（1）是避免特征因为和目标值非线性关系带来的影响；  
>>>（2）离散化也是种给lr线性模型带来非线性的一种方法；  
>>>（3）方便引入交叉特征；  
>>>（4）工程实现上的trick。
>>>常见的离散化处理手法：非监督的方法和监督的方法。非监督的方法：等宽，等频，经验，分布图划分等。监督方法：基于信息增益或卡方检验的区间分裂算法和基于信息增益或卡方检验的区间合并算法等。
    在风控采用lr模型的时候，对于连续特征采用离散化处理会有个这样的问题：因为我们的坏样本是针对过去的欺诈场景的，欺诈手法在长期博弈中不断升级。我们不仅要让模型尽可能多的覆盖过去的欺诈手法，对未来产生欺诈对抗有一定的适应性，不至于失效太快。采用离散化处理后，就可能出现很大的跳变性。假设我们过去的的坏样本都是刚注册不久的用户，那注册时间间隔做离散化处理时候，就可能分为A，B两段，离散化处理后可以看成0-1二值变量，落在A段为1，否则为0。为1时候风险高，权重为正值。如果这个变量在过去对正负样本区分度很高，可以看成核心变量的话，那如果骗子绕过A段，跳到B段的话，对模型的预测能力衰弱会是致命的。  

> **3、特征筛选的常用方法：**  
>> **1）信息值(information value)：** 简称IV值；IV值越大，重要程度越高。  
>> **2）信息增益(information gain)：** 是采用信息熵的方法，信息增益表示信息熵的变化， 增益越大，说明特征区分度越明显。  
>> **3）前向后向选择，依赖模型，通过AIC或者BIC来选择最优特征集合。**  
>> **4）基于其他分类模型的特征选择。**  
>> **5）基于业务的选择方法：**1.字段是否容易获取。2.字段获取渠道是否稳定。3.字段本身是否足够稳定。  

## 三、模型选择  
> 风控的意图，简单来说就是排除坏人和挑出好人，也就是一个二分类问题。  

>> 首先，既然是二分类问题，我们就有很多的模型可以选择。以小额贷款为例。因经常需要给客户不予以贷款的缘由，故对模型的可解释性要求比较高，通常会选择逻辑回归模型、决策树模型等模型。  
>> 而在该领域内的数据竞赛中，高分选手最常用的模型是：特征工程+ XGBoost，LightGBM，GBDT+ 集成。大量的实践比赛已证明这三个模型的效果非常理想，若进步加权集成，效果将更加显著，远胜Kmeans、Logistic Regression、SVM等老牌模型。几乎所有的非图像、文本的数据竞赛领域的获奖选手都会使用。  
>> 其次，虽然是做简单的二分类，但是依然可以根据用途来挑选不同的模型。  
